{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a57b63",
   "metadata": {},
   "source": [
    "# 1. Dividing text into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf1440",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f732bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae74f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/sherlock_holmes_1.txt'\n",
    "file = open(filename, 'r', encoding='utf-8')\n",
    "text = file.read()\n",
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d79ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f393fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him mention her under any other name.', 'In his eyes she eclipses and predominates the whole of her sex.', 'It was not that he felt any emotion akin to love for Irene Adler.', 'All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.', 'He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.', 'He never spoke of the softer passions, save with a gibe and a sneer.', 'They were admirable things for the observer—excellent for drawing the veil from men’s motives and actions.', 'But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results.', 'Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his.', 'And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.']\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenizer.tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06342fd",
   "metadata": {},
   "source": [
    "###  Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a29671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a9dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da5a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/sherlock_holmes_1.txt'\n",
    "file = open(filename, 'r', encoding='utf-8')\n",
    "text = file.read()\n",
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629b2d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him mention her under any other name.', 'In his eyes she eclipses and predominates the whole of her sex.', 'It was not that he felt any emotion akin to love for Irene Adler.', 'All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.', 'He was, I take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position.', 'He never spoke of the softer passions, save with a gibe and a sneer.', 'They were admirable things for the observer—excellent for drawing the veil from men’s motives and actions.', 'But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results.', 'Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his.', 'And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "sentences = [sentence.text for sentence in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7872b7",
   "metadata": {},
   "source": [
    "###  More complicated example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ef0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'If you’re hungry at midnight, you can go to 7-11, FamilyMart, etc. \\\n",
    "I established my own workshop in 2018, i.e., two years ago. \\\n",
    "Email me the menu at abc-123@gmai.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35d81be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you’re hungry at midnight, you can go to 7-11, FamilyMart, etc.', 'I established my own workshop in 2018, i.e., two years ago.', 'Email me the menu at abc-123@gmai.com']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(s)\n",
    "sentences = [sentence.text for sentence in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7deab758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you’re hungry at midnight, you can go to 7-11, FamilyMart, etc.', 'I established my own workshop in 2018, i.e., two years ago.', 'Email me the menu at abc-123@gmai.com']\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenizer.tokenize(s)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e425f0",
   "metadata": {},
   "source": [
    "# 2. Dividing sentences into words – tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0116bc8",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f9011f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', '’', 're', 'hungry', 'at', 'midnight', ',', 'you', 'can', 'go', 'to', '7-11', ',', 'FamilyMart', ',', 'etc', '.', 'I', 'established', 'my', 'own', 'workshop', 'in', '2018', ',', 'i.e.', ',', 'two', 'years', 'ago', '.', 'Email', 'me', 'the', 'menu', 'at', 'abc-123', '@', 'gmai.com']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(s)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d575d",
   "metadata": {},
   "source": [
    "### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0863302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', '’re', 'hungry', 'at', 'midnight', ',', 'you', 'can', 'go', 'to', '7', '-', '11', ',', 'FamilyMart', ',', 'etc', '.', 'I', 'established', 'my', 'own', 'workshop', 'in', '2018', ',', 'i.e.', ',', 'two', 'years', 'ago', '.', 'Email', 'me', 'the', 'menu', 'at', 'abc-123@gmai.com']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(s)\n",
    "words = [token.text for token in doc]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414489e",
   "metadata": {},
   "source": [
    "### Using spaCy for Chinese Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa59b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['超級', '1000', '系列', '全', '英', '公開', '賽將', '於', '3月', '16日', '登場', '，', '昨', '（', '22日', '）', '籤表', '出爐', '，', '我', '國', '世界', '球后', '戴', '資穎', '仍', '以', '第一', '種子', '出戰', '，', '尋求', '個人', '在', '全', '英', '公開', '賽', '的', '第四', '座', '冠軍', '。', '印度', '媒體', '《', '滾動', '》', '則', '報導', '，', '「', '小戴', '」', '籤運不錯', '，', '如果', '能', '穩定', '發揮', '，', '晉級', '八', '強', '不是', '問題', '。']\n"
     ]
    }
   ],
   "source": [
    "zh_nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "zh_s = \"超級1000系列全英公開賽將於3月16日登場，\\\n",
    "昨（22日）籤表出爐，我國世界球后戴資穎仍以第一種子出戰，\\\n",
    "尋求個人在全英公開賽的第四座冠軍。印度媒體《滾動》則報導，\\\n",
    "「小戴」籤運不錯，如果能穩定發揮，晉級八強不是問題。\"\n",
    "\n",
    "doc = zh_nlp(zh_s)\n",
    "words = [token.text for token in doc]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801b962",
   "metadata": {},
   "source": [
    "### NLTK's special tokenizer for tweets\n",
    "#### ref: https://www.nltk.org/api/nltk.tokenize.casual.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a745106",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@EmpireStateBldg Central Park Tower is reaaaally hiiiigh :-) ^^ <3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dea83e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Central', 'Park', 'Tower', 'is', 'reaaally', 'hiiigh', ':-)', '^', '^', '<3']\n"
     ]
    }
   ],
   "source": [
    "words = \\\n",
    "nltk.tokenize.casual.casual_tokenize(tweet,\n",
    "preserve_case=True,\n",
    "reduce_len=True,\n",
    "strip_handles=True)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7dc1ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@EmpireStateBldg', 'Central', 'Park', 'Tower', 'is', 'reaaally', 'hiiigh', ':-)', '^', '^', '<3']\n"
     ]
    }
   ],
   "source": [
    "words = \\\n",
    "nltk.tokenize.casual.casual_tokenize(tweet,\n",
    "preserve_case=True,\n",
    "reduce_len=True,\n",
    "strip_handles=False)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746a1bc",
   "metadata": {},
   "source": [
    "# Parts of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f728166",
   "metadata": {},
   "source": [
    "### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31afe48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ed76804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_pos(result):\n",
    "    words = [token.text for token in result]\n",
    "    pos = [token.pos_ for token in result]\n",
    "    word_pos_tuples = list(zip(words, pos))\n",
    "    return word_pos_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e635ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('They', 'PRON'), ('can', 'AUX'), ('fish', 'VERB'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "example = \"They can fish.\"\n",
    "result = nlp(example)\n",
    "word_pos_tuples = ext_pos(result)\n",
    "\n",
    "print(word_pos_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98f102fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('If', 'SCONJ'), ('you', 'PRON'), ('’re', 'VERB'), ('hungry', 'ADJ'), ('at', 'ADP'), ('midnight', 'NOUN'), (',', 'PUNCT'), ('you', 'PRON'), ('can', 'AUX'), ('go', 'VERB'), ('to', 'ADP'), ('7', 'NUM'), ('-', 'SYM'), ('11', 'NUM'), (',', 'PUNCT'), ('FamilyMart', 'PROPN'), (',', 'PUNCT'), ('etc', 'X'), ('.', 'PUNCT'), ('I', 'PRON'), ('established', 'VERB'), ('my', 'PRON'), ('own', 'ADJ'), ('workshop', 'NOUN'), ('in', 'ADP'), ('2018', 'NUM'), (',', 'PUNCT'), ('i.e.', 'X'), (',', 'PUNCT'), ('two', 'NUM'), ('years', 'NOUN'), ('ago', 'ADV'), ('.', 'PUNCT'), ('We', 'PRON'), ('sell', 'VERB'), ('various', 'ADJ'), ('electronic', 'ADJ'), ('devices', 'NOUN'), (',', 'PUNCT'), ('e.g.', 'ADV'), (',', 'PUNCT'), ('computers', 'NOUN'), (',', 'PUNCT'), ('fans', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('heaters', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "s = 'If you’re hungry at midnight, you can go to 7-11, FamilyMart, etc. \\\n",
    "I established my own workshop in 2018, i.e., two years ago. \\\n",
    "We sell various electronic devices, e.g., computers, fans, and heaters.'\n",
    "\n",
    "result = nlp(s)\n",
    "word_pos_tuples = ext_pos(result)\n",
    "print(word_pos_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ba9dee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('超級', 'VERB'), ('1000', 'NUM'), ('系列', 'NUM'), ('全', 'DET'), ('英', 'PROPN'), ('公開', 'NOUN'), ('賽將', 'ADV'), ('於', 'ADP'), ('3月', 'NOUN'), ('16日', 'NOUN'), ('登場', 'VERB'), ('，', 'PUNCT'), ('昨', 'NOUN'), ('（', 'PUNCT'), ('22日', 'NUM'), ('）', 'PUNCT'), ('籤表', 'NOUN'), ('出爐', 'VERB'), ('，', 'PUNCT'), ('我', 'PRON'), ('國', 'VERB'), ('世界', 'NOUN'), ('球后', 'NOUN'), ('戴', 'VERB'), ('資穎', 'NOUN'), ('仍', 'ADV'), ('以', 'ADP'), ('第一', 'NUM'), ('種子', 'NOUN'), ('出戰', 'VERB'), ('，', 'PUNCT'), ('尋求', 'NOUN'), ('個人', 'VERB'), ('在', 'ADP'), ('全', 'DET'), ('英', 'PROPN'), ('公開', 'NOUN'), ('賽', 'VERB'), ('的', 'PART'), ('第四', 'NUM'), ('座', 'NUM'), ('冠軍', 'NOUN'), ('。', 'PUNCT'), ('印度', 'PROPN'), ('媒體', 'NOUN'), ('《', 'PUNCT'), ('滾動', 'NOUN'), ('》', 'PUNCT'), ('則', 'VERB'), ('報導', 'NOUN'), ('，', 'PUNCT'), ('「', 'PUNCT'), ('小戴', 'NOUN'), ('」', 'PUNCT'), ('籤運不錯', 'VERB'), ('，', 'PUNCT'), ('如果', 'SCONJ'), ('能', 'VERB'), ('穩定', 'VERB'), ('發揮', 'VERB'), ('，', 'PUNCT'), ('晉級', 'VERB'), ('八', 'NUM'), ('強', 'NUM'), ('不是', 'ADV'), ('問題', 'NOUN'), ('。', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "zh_s = \"超級1000系列全英公開賽將於3月16日登場，\\\n",
    "昨（22日）籤表出爐，我國世界球后戴資穎仍以第一種子出戰，\\\n",
    "尋求個人在全英公開賽的第四座冠軍。印度媒體《滾動》則報導，\\\n",
    "「小戴」籤運不錯，如果能穩定發揮，晉級八強不是問題。\"\n",
    "\n",
    "zh_nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "result = zh_nlp(zh_s)\n",
    "word_pos_tuples = ext_pos(result)\n",
    "print(word_pos_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4145d1d5",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea0c5e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('If', 'IN'), ('you', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('hungry', 'NN'), ('at', 'IN'), ('midnight', 'NN'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('go', 'VB'), ('to', 'TO'), ('7-11', 'CD'), (',', ','), ('FamilyMart', 'NNP'), (',', ','), ('etc', 'FW'), ('.', '.'), ('I', 'PRP'), ('established', 'VBD'), ('my', 'PRP$'), ('own', 'JJ'), ('workshop', 'NN'), ('in', 'IN'), ('2018', 'CD'), (',', ','), ('i.e.', 'FW'), (',', ','), ('two', 'CD'), ('years', 'NNS'), ('ago', 'RB'), ('.', '.'), ('We', 'PRP'), ('sell', 'VBP'), ('various', 'JJ'), ('electronic', 'JJ'), ('devices', 'NNS'), (',', ','), ('e.g.', 'NN'), (',', ','), ('computers', 'NNS'), (',', ','), ('fans', 'NNS'), (',', ','), ('and', 'CC'), ('heaters', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "words = nltk.tokenize.word_tokenize(s)\n",
    "words_with_pos = nltk.pos_tag(words)\n",
    "\n",
    "print(words_with_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d70f2d",
   "metadata": {},
   "source": [
    "# Word stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae1aad",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9d4c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['leaf', 'leaves', 'booking', 'writing',\n",
    "'completed', 'stemming', 'skies', 'gone', 'goes', 'this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05d8f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leaf', 'leav', 'book', 'write', 'complet', 'stem', 'sky', 'gone', 'goe', 'this']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95f2614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leaf', 'leav', 'book', 'write', 'complet', 'stem', 'sky', 'gone', 'goe', 'thi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "stemmed_words = [porterStemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "196973e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leaf', 'leav', 'book', 'writ', 'complet', 'stem', 'ski', 'gon', 'goe', 'thi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "lancasterStemmer = LancasterStemmer()\n",
    "stemmed_words = [lancasterStemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe63d5a",
   "metadata": {},
   "source": [
    "# Combining similar words – lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b265f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This this\n",
      "product product\n",
      "integrates integrate\n",
      "both both\n",
      "libraries library\n",
      "for for\n",
      "downloading download\n",
      "and and\n",
      "applying apply\n",
      "patches patch\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp('This product integrates both libraries for downloading and applying patches.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348c405",
   "metadata": {},
   "source": [
    "# Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e16c833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e52dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', '’', 're', 'hungry', 'at', 'midnight', ',', 'you', 'can', 'go', 'to', '7-11', ',', 'FamilyMart', ',', 'etc', '.', 'I', 'established', 'my', 'own', 'workshop', 'in', '2018', ',', 'i.e.', ',', 'two', 'years', 'ago', '.', 'We', 'sell', 'various', 'electronic', 'devices', ',', 'e.g.', ',', 'computers', ',', 'fans', ',', 'and', 'heaters', '.']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(s)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f12fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['’', 'hungry', 'midnight', ',', 'go', '7-11', ',', 'FamilyMart', ',', 'etc', '.', 'established', 'workshop', '2018', ',', 'i.e.', ',', 'two', 'years', 'ago', '.', 'sell', 'various', 'electronic', 'devices', ',', 'e.g.', ',', 'computers', ',', 'fans', ',', 'heaters', '.']\n"
     ]
    }
   ],
   "source": [
    "words = [word for word in words if word.lower() not in stopwords]\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e21aa",
   "metadata": {},
   "source": [
    "# Getting the dependency parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc0bc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://alvinntnu.github.io/python-notes/nlp/nlp-spacy-zh.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8af2ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'I need a ticket to Los Angeles on May 8th.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9eed15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tPRON\tnsubj\tnominal subject\tneed\n",
      "need\tVERB\tROOT\tNone\tneed\n",
      "a\tDET\tdet\tdeterminer\tticket\n",
      "ticket\tNOUN\tdobj\tdirect object\tneed\n",
      "to\tADP\tprep\tprepositional modifier\tticket\n",
      "Los\tPROPN\tcompound\tcompound\tAngeles\n",
      "Angeles\tPROPN\tpobj\tobject of preposition\tto\n",
      "on\tADP\tprep\tprepositional modifier\tneed\n",
      "May\tPROPN\tcompound\tcompound\t8th\n",
      "8th\tNOUN\tpobj\tobject of preposition\ton\n",
      ".\tPUNCT\tpunct\tpunctuation\tneed\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sent)\n",
    "\n",
    "for token in doc:\n",
    "    print('%s\\t%s\\t%s\\t%s\\t%s' %(token.text, token.pos_, token.dep_, \\\n",
    "                        spacy.explain(token.dep_), token.head.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "679148e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los GPE\n",
      "Angeles GPE\n",
      "May DATE\n",
      "8th DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sent)\n",
    "\n",
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print(token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b3af69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "nearly MONEY\n",
      "$ MONEY\n",
      "100 MONEY\n",
      "million MONEY\n",
      "Tim PERSON\n",
      "Cook PERSON\n"
     ]
    }
   ],
   "source": [
    "ner_s = 'Apple investors urged to vote against a nearly \\\n",
    "$100 million pay package for CEO Tim Cook.'\n",
    "\n",
    "doc = nlp(ner_s)\n",
    "\n",
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print(token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500c919",
   "metadata": {},
   "source": [
    "# Extracting noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da927121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple investors\n",
      "a nearly $100 million pay package\n",
      "CEO\n",
      "Tim Cook\n"
     ]
    }
   ],
   "source": [
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433152a",
   "metadata": {},
   "source": [
    "# Extracting subjects, predicates, and objects of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f708613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\tPRON\tnsubj\tnominal subject\testablished\t1\n",
      "established\tVERB\tROOT\tNone\testablished\t1\n",
      "my\tPRON\tposs\tpossession modifier\tworkshop\t4\n",
      "own\tADJ\tamod\tadjectival modifier\tworkshop\t4\n",
      "workshop\tNOUN\tdobj\tdirect object\testablished\t1\n",
      "in\tADP\tprep\tprepositional modifier\testablished\t1\n",
      "2018\tNUM\tpobj\tobject of preposition\tin\t5\n",
      "before\tSCONJ\tmark\tmarker\twent\t9\n",
      "I\tPRON\tnsubj\tnominal subject\twent\t9\n",
      "went\tVERB\tadvcl\tadverbial clause modifier\testablished\t1\n",
      "to\tADP\tprep\tprepositional modifier\twent\t9\n",
      "Japan\tPROPN\tpobj\tobject of preposition\tto\t10\n",
      ".\tPUNCT\tpunct\tpunctuation\testablished\t1\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I established my own workshop in 2018 before I went to Japan.'\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print('%s\\t%s\\t%s\\t%s\\t%s\\t%d' %(token.text, token.pos_, token.dep_, \\\n",
    "                        spacy.explain(token.dep_), token.head.text, token.head.i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d30c0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I [I]\n",
      "established [I, established, my, own, workshop, in, 2018, before, I, went, to, Japan, .]\n",
      "my [my]\n",
      "own [own]\n",
      "workshop [my, own, workshop]\n",
      "in [in, 2018]\n",
      "2018 [2018]\n",
      "before [before]\n",
      "I [I]\n",
      "went [before, I, went, to, Japan]\n",
      "to [to, Japan]\n",
      "Japan [Japan]\n",
      ". [.]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    subtree = list(token.subtree)\n",
    "    print(token, subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "998f740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, established), (9, went)]\n"
     ]
    }
   ],
   "source": [
    "verb_idxs = [(i, token) for i, token in enumerate(doc) if token.pos_ == 'VERB']\n",
    "print(verb_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "052c0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase(doc, head_idx, tag):\n",
    "    for token in doc:\n",
    "        if tag in token.dep_ and token.head.i == head_idx:\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c185a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: I\n",
      "predicate: established\n",
      "object: my own workshop\n",
      "subject: I\n",
      "predicate: went\n",
      "object: None\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sentence)\n",
    "\n",
    "for verb_idx in verb_idxs:\n",
    "    subject_phrase = get_phrase(doc, verb_idx[0], 'subj')\n",
    "    object_phrase = get_phrase(doc, verb_idx[0], 'obj')\n",
    "    print('subject:', subject_phrase)\n",
    "    print('predicate:', doc[verb_idx[0]])\n",
    "    print('object:', object_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d777b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
